{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618641d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VIN DETECT & RECOGNISE ###\n",
    "##VIN Detection - YOLO v5\n",
    "##VIN Extraction\n",
    "##Image Converion\n",
    "##VIN recognition - PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e705937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddle_man(img_path):\n",
    "    from paddleocr import PaddleOCR,draw_ocr\n",
    "    from PIL import ImageFont\n",
    "    import PIL\n",
    "    ocr = PaddleOCR(det_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/ch_PP-OCRv2_det_distill_train', rec_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/ret_model/ch_PP-OCRv2_rec_train',cls_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/cls_model/ch_ppocr_mobile_v2.0_cls_train',use_angle_cls=True) \n",
    "    result = ocr.ocr(img_path,cls=True)\n",
    "    txts = [line[1][0] for line in result]\n",
    "    scores = [line[1][1] for line in result]\n",
    "    \n",
    "    # draw result\n",
    "    from PIL import Image\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    boxes = [line[0] for line in result]\n",
    "    txts = [line[1][0] for line in result]\n",
    "    scores = [line[1][1] for line in result]\n",
    "    font = ImageFont.load_default()\n",
    "    \n",
    "    im_show = draw_ocr(image, boxes, txts, scores, font_path='L:/CV/PaddleOCR/PaddleOCR/doc/fonts/simfang.ttf')\n",
    "    im_show = Image.fromarray(im_show)\n",
    "    im_show.save('result.jpg')\n",
    "    im = Image.open(\"result.jpg\")\n",
    "    im.show() \n",
    "    return txts,scores\n",
    "\n",
    "def display(im_path):\n",
    "    dpi = 80\n",
    "    im_data = plt.imread(im_path)    \n",
    "    height, width  = im_data.shape[:2]\n",
    "    \n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)    \n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])    \n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')    \n",
    "    # Display the image.\n",
    "    ax.imshow(im_data, cmap='gray')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7dc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/28816046/\n",
    "#displaying-different-images-with-actual-size-in-matplotlib-subplot\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "def display(im_path):\n",
    "    dpi = 80\n",
    "    im_data = plt.imread(im_path)    \n",
    "    height, width  = im_data.shape[:2]\n",
    "    \n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)    \n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])    \n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')    \n",
    "    # Display the image.\n",
    "    ax.imshow(im_data, cmap='gray')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26c1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_code(img):\n",
    "    ##Libraries\n",
    "    import os\n",
    "    import cv2\n",
    "    import shutil\n",
    "    \n",
    "    #Remove previous results\n",
    "    isdir = os.path.isdir('runs/detect')\n",
    "    if isdir == True:\n",
    "        shutil.rmtree(r'runs/detect')\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #Convert image to gray scale\n",
    "    \n",
    "    read_img = cv2.imread(img)\n",
    "    gray_image=cv2.cvtColor(read_img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite('temp/gray_image.jpg', gray_image)\n",
    "    \n",
    "    #Detect VIN Region\n",
    "    !python yolov5/detect.py --weights yolov5/yolov5/weights/large/best.pt --img 416 --conf 0.6 --source temp/gray_image.jpg --project runs/detect/ --name exp --save-crop\n",
    "    #import torch\n",
    "    #model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/yolov5/weights/best.pt')  # default\n",
    "    #imgs = ['temp/gray_image.jpg'] \n",
    "    #results = model(imgs)\n",
    "    #results.xyxy[0]  \n",
    "    #results.pandas().xyxy[0]  \n",
    "    #crops = results.crop(save=True)\n",
    "    \n",
    "    \n",
    "    ##Check if directry present(True if object detection is correct)\n",
    "    detect_folder='runs/detect/exp/crops/Letters/'\n",
    "    isdir = os.path.isdir(detect_folder)\n",
    "    if isdir == False:\n",
    "        txts,score=paddle_man(img)   \n",
    "    else:\n",
    "        print('Dir exist')\n",
    "        file = os.listdir(detect_folder)\n",
    "        file=file[0]\n",
    "        path = (detect_folder+\"\"+file)\n",
    "        src = cv2.imread(path)\n",
    "        image = cv2.rotate(src, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        cv2.imwrite(\"runs/temp/\"+file, image)    \n",
    "        #Convert to inverted imge\n",
    "        gray_image_file='runs/temp/'+ file\n",
    "        image_read=cv2.imread(gray_image_file)\n",
    "        inverted_image = cv2.bitwise_not(image_read)\n",
    "        cv2.imwrite(\"runs/temp/inverted.jpg\", inverted_image)\n",
    "        #Pass images to OCR\n",
    "        txts,score=paddle_man(\"runs/temp/inverted.jpg\")\n",
    "        \n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    return txts,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bd2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    " import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befb2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def liveness_det_one(img_in):\n",
    "    ##Livesness detection\n",
    "    ##Datasouls\n",
    "    import cv2\n",
    "    %matplotlib inline\n",
    "    from pylab import imshow\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import torch\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import albumentations as albu\n",
    "    from albumentations.pytorch.transforms import ToTensorV2\n",
    "    from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image\n",
    "    from iglovikov_helper_functions.utils.image_utils import load_rgb\n",
    "    from datasouls_antispoof.pre_trained_models import create_model\n",
    "    from datasouls_antispoof.class_mapping import class_mapping\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    #model = create_model(\"tf_efficientnet_b3_ns\")\n",
    "    model1 = create_model(\"tf_efficientnet_b3_ns\")\n",
    "    model2 = create_model(\"swsl_resnext50_32x4d\")\n",
    "    model1.eval();\n",
    "    model2.eval();\n",
    "    image_replay = load_rgb(img)\n",
    "    #imshow(image_replay)\n",
    "    ###\n",
    "    transform = albu.Compose([albu.PadIfNeeded(min_height=400, min_width=400),\n",
    "                              albu.CenterCrop(height=400, width=400), \n",
    "                              albu.Normalize(p=1), \n",
    "                              albu.pytorch.ToTensorV2(p=1)], p=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction1 = model1(torch.unsqueeze(transform(image=image_replay)['image'], 0)).numpy()[0]\n",
    "        prediction2 = model2(torch.unsqueeze(transform(image=image_replay)['image'], 0)).numpy()[0]\n",
    "\n",
    "    df1 = pd.DataFrame({\"prediction\": prediction1, \"class_name\": class_mapping.keys()})\n",
    "    df2 = pd.DataFrame({\"prediction\": prediction2, \"class_name\": class_mapping.keys()})\n",
    "    #sns.barplot(data=df1, x=\"prediction\", y=\"class_name\") \n",
    "    #sns.barplot(data=df2, x=\"prediction\", y=\"class_name\") \n",
    "    \n",
    "    res1_df=df1.loc[df1['prediction'].idxmax()]\n",
    "    res2_df=df2.loc[df2['prediction'].idxmax()]\n",
    "    redf1,redf2,pred1,pred2=res1_df.class_name,res2_df.class_name,res1_df.prediction,res2_df.prediction\n",
    "    ###\n",
    "    return redf1,redf2,pred1,pred2\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Livesness detection\n",
    "    ##Neural Network(Binary classification)\n",
    "\n",
    "def liveness_det_two(img_in):\n",
    "    from imutils.video import VideoStream\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import argparse\n",
    "    import imutils\n",
    "    import pickle\n",
    "    import time\n",
    "    import cv2\n",
    "    import os\n",
    "    \n",
    "    liveness_model = tf.keras.models.load_model('LIVENESS_DETECT/face_liveness_detection/liveness_model_two.h5 ')\n",
    "    le = pickle.loads(open('LIVENESS_DETECT/face_liveness_detection/label_encoder','rb').read())\n",
    "    \n",
    "    ##Read image and resize --\n",
    "    frame = cv2.imread(img)\n",
    "    face = cv2.resize(frame, (128,128))\n",
    "    \n",
    "    face = face.astype('float') / 255.0 \n",
    "    face = tf.keras.preprocessing.image.img_to_array(face)\n",
    "    # tf model require batch of data to feed in\n",
    "    # so if we need only one image at a time, we have to add one more dimension\n",
    "    # in this case it's the same with [face]\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    \n",
    "    preds = liveness_model.predict(face)[0]\n",
    "    j = np.argmax(preds)\n",
    "    label = le.classes_[j]\n",
    "    result2= 'fake' if label ==1 else 'real'\n",
    "    res3df=pd.DataFrame(preds,columns=['dd'])\n",
    "    tmp_df=res3df.loc[res3df['dd'].idxmax()]\n",
    "    pred3=tmp_df.dd\n",
    "    return result2,pred3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1618c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317d9dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir exist\n",
      "[2022/02/16 10:21:08] root WARNING: version PP-OCRv2 not support cls models, auto switch to version PP-OCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5/yolov5/weights/large/best.pt'], source=temp/gray_image.jpg, imgsz=[416, 416], conf_thres=0.6, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect/, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.0-131-gc77a5a8 torch 1.10.0 CUDA:0 (GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients, 107.8 GFLOPs\n",
      "image 1/1 L:\\CV\\MAIN_PRO_POC\\temp\\gray_image.jpg: 416x256 1 Letters, Done. (0.021s)\n",
      "Speed: 0.0ms pre-process, 21.0ms inference, 4.0ms NMS per image at shape (1, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns\\detect\\exp\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(benchmark=False, cls_batch_num=6, cls_image_shape='3, 48, 192', cls_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/cls_model/ch_ppocr_mobile_v2.0_cls_train', cls_thresh=0.9, cpu_threads=10, det=True, det_algorithm='DB', det_db_box_thresh=0.6, det_db_score_mode='fast', det_db_thresh=0.3, det_db_unclip_ratio=1.5, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type='max', det_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/ch_PP-OCRv2_det_distill_train', det_pse_box_thresh=0.85, det_pse_box_type='box', det_pse_min_area=16, det_pse_scale=1, det_pse_thresh=0, det_sast_nms_thresh=0.2, det_sast_polygon=False, det_sast_score_thresh=0.5, drop_score=0.5, e2e_algorithm='PGNet', e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_limit_side_len=768, e2e_limit_type='max', e2e_model_dir=None, e2e_pgnet_mode='fast', e2e_pgnet_polygon=True, e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set='totaltext', enable_mkldnn=False, gpu_mem=500, help='==SUPPRESS==', image_dir=None, ir_optim=True, label_list=['0', '180'], lang='ch', layout_path_model='lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config', max_batch_size=10, max_text_length=25, min_subgraph_size=15, ocr_version='PP-OCRv2', output='./output/table', precision='fp32', process_id=0, rec=True, rec_algorithm='CRNN', rec_batch_num=6, rec_char_dict_path='L:\\\\ANACONDA\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\ppocr_keys_v1.txt', rec_image_shape='3, 32, 320', rec_model_dir='paddleOCR_V2_EN_GEN_SRV_MOB_T/ret_model/ch_PP-OCRv2_rec_train', save_log_path='./log_output/', show_log=True, structure_version='STRUCTURE', table_char_dict_path=None, table_char_type='en', table_max_len=488, table_model_dir=None, total_process_num=1, type='ocr', use_angle_cls=True, use_dilation=False, use_gpu=False, use_mp=False, use_onnx=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, vis_font_path='./doc/fonts/simfang.ttf', warmup=True)\n",
      "[2022/02/16 10:21:08] root DEBUG: dt_boxes num : 1, elapse : 0.08401823043823242\n",
      "[2022/02/16 10:21:08] root DEBUG: cls num  : 1, elapse : 0.013002872467041016\n",
      "[2022/02/16 10:21:08] root DEBUG: rec_res num  : 1, elapse : 0.07701778411865234\n",
      "\n",
      "\n",
      "Recognised Text : []\n",
      "Accuracy : []\n",
      "\n",
      "Image Liveness Detection Result:\n",
      "\n",
      "Model1 : real  --> Accuracy: 0.74563205\n",
      "Model2 : real  --> Accuracy: 0.5559545\n",
      "Modle3 : real  --> Accuracy: 0.8874323\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "\n",
    "folder='test/HERO_PRINTED/real/'\n",
    "file='real7.jpeg'\n",
    "img=(folder+\"\"+file)\n",
    "#display(img)\n",
    "txts,score=main_code(img);\n",
    "redf1,redf2,pred1,pred2=liveness_det_one(img);\n",
    "liv3,pred3=liveness_det_two(img);\n",
    "\n",
    "\n",
    "###Printing Results\n",
    "print(\"\\n\")\n",
    "print(\"Recognised Text :\",txts)\n",
    "print(\"Accuracy :\",score)\n",
    "print()\n",
    "###Liveness Detection Results###\n",
    "##Model1 & Model2\n",
    "print(\"Image Liveness Detection Result:\\n\")\n",
    "print(\"Model1 :\",redf1,\" --> Accuracy:\",pred1)\n",
    "print(\"Model2 :\",redf2,\" --> Accuracy:\",pred2)\n",
    "\n",
    "##Model3\n",
    "print(\"Modle3 :\",liv3,\" --> Accuracy:\",pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d765cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecbafe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Liveness Detection Result:\n",
      "\n",
      "Model1 : real  --> Accuracy: 0.5863823\n",
      "Model2 : replay  --> Accuracy: 0.63051635\n",
      "Modle3 : fake  --> Accuracy: 0.9946108\n"
     ]
    }
   ],
   "source": [
    "##Liveness DetectOOnly###\n",
    "##Liveness DetectOOnly###\n",
    "\n",
    "#Variables\n",
    "\n",
    "folder='test/HERO_PRINTED/real/'\n",
    "file='real1.jpeg'\n",
    "img=(folder+\"\"+file)\n",
    "#display(img)\n",
    "redf1,redf2,pred1,pred2=liveness_det_one(img);\n",
    "liv3,pred3=liveness_det_two(img);\n",
    "\n",
    "###Liveness Detection Results###\n",
    "##Model1 & Model2\n",
    "print(\"Image Liveness Detection Result:\\n\")\n",
    "print(\"Model1 :\",redf1,\" --> Accuracy:\",pred1)\n",
    "print(\"Model2 :\",redf2,\" --> Accuracy:\",pred2)\n",
    "\n",
    "##Model3\n",
    "print(\"Modle3 :\",liv3,\" --> Accuracy:\",pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cecc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c2d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e257c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8847d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9d66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab811dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models:\n",
    "    1.YOLO V5 - Object detection(ROI)  -->Custom Dataset(1000 Images)\n",
    "    2.OCR Mode - Text recognition      -->Pre trained\n",
    "    \n",
    "    3.Liveness Detection - Model1 (Replay/printed) -->Pre trained\n",
    "    4.Liveness Detection - Model2 (Replay/printed) -->Pre trained\n",
    "    5.Liveness Detection - Model3 (Replay)         -->Custom Dataset(200 replay Images)\n",
    "\n",
    "    \n",
    "Liveness Detection(Spoofing methods):\n",
    "    1.Replay  (Images captured from computer/tab/phone screens)\n",
    "    2.Printed (Images captured from printed papers)\n",
    "    \n",
    "Dataset: \n",
    "    1.replay  - 2000+ Images\n",
    "    2.printed - 2000+ images\n",
    "\n",
    "------------------------------------------------\n",
    "    \n",
    "OCR Custom Model for VIN Detection:\n",
    "    1.Labelling: \n",
    "    2.Training : \n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a224c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Requirements:\n",
    "    VIN recognition\n",
    "    Replay image detection\n",
    "    printed image dettection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63da49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3a8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from paddleocr import PaddleOCR,draw_ocr\n",
    "    from PIL import ImageFont\n",
    "    import PIL\n",
    "    from PIL import Image\n",
    "    import cv2\n",
    "    from matplotlib import pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import os\n",
    "    import shutil\n",
    "    import pandas as pd\n",
    "    from pylab import imshow\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import seaborn as sns\n",
    "    import albumentations as albu\n",
    "    from albumentations.pytorch.transforms import ToTensorV2\n",
    "    from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image\n",
    "    from iglovikov_helper_functions.utils.image_utils import load_rgb\n",
    "    from datasouls_antispoof.pre_trained_models import create_model\n",
    "    from datasouls_antispoof.class_mapping import class_mapping\n",
    "    from imutils.video import VideoStream\n",
    "    import tensorflow as tf\n",
    "    import argparse\n",
    "    import imutils\n",
    "    import pickle\n",
    "    import time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f46714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560f7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
